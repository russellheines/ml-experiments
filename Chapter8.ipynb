{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTkVLZbrevphkovKGjNkXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russellheines/ml-experiments/blob/main/Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2 Downloading and parsing the initial text file"
      ],
      "metadata": {
        "id": "ByJHTFjIZEcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca7Y_S4DZA34",
        "outputId": "f835d65d-aecd-4f47-ecab-9adc85858de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 600893\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "path = keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = open(path).read().lower()\n",
        "print('Corpus length:', len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.3 Vectorizing sequences of characters"
      ],
      "metadata": {
        "id": "mUPJMf_CaDJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 60\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sentences.append(text[i: i + maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        "\n",
        "print('Number of sequences:', len(sentences))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characteres:', len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t80eDWJoZh-C",
        "outputId": "6e11d65a-9c02-481f-d34f-562534190794"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 200278\n",
            "Unique characteres: 57\n",
            "Vectorization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ca748dc5ccd1>:17: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
            "<ipython-input-3-ca748dc5ccd1>:18: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.4 Single-later LSTM model for next-character prediction"
      ],
      "metadata": {
        "id": "EjQSfp72bXS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))"
      ],
      "metadata": {
        "id": "3hBX7_jdaGCT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.5 Model compilation configuration"
      ],
      "metadata": {
        "id": "8VtS3tXjbzFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL4gVKc5bvEs",
        "outputId": "42160511-a241-48dc-a035-0415f68a4b96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.6 Function to sample the next character given the model's predictions"
      ],
      "metadata": {
        "id": "Ap6T4rEFLPxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "metadata": {
        "id": "tJI101DNb7Ds"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.7 Text generation loop"
      ],
      "metadata": {
        "id": "E3upLGnyL4N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 60):\n",
        "  print('epoch', epoch)\n",
        "  model.fit(x, y, batch_size=128, epochs=1)\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  generated_text = text[start_index: start_index + maxlen]\n",
        "  print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "  for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print('------ temperature:', temperature)\n",
        "    sys.stdout.write(generated_text)\n",
        "\n",
        "    for i in range(400):\n",
        "      sampled = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(generated_text):\n",
        "        sample[0, t, char_indices[char]] = 1\n",
        "\n",
        "      preds = model.predict(sampled, verbose=0)[0]\n",
        "      next_index = sample(preds, temperature)\n",
        "      next_char = chars[next_index]\n",
        "\n",
        "      generated_text += next_char\n",
        "      generated_text = generated_text[1:]\n",
        "\n",
        "      sys.stdout.write(next_char)"
      ],
      "metadata": {
        "id": "YtbR-C40L1yB",
        "outputId": "890f6b48-c88e-4f60-cf30-e0ac50c05541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "1565/1565 [==============================] - 271s 173ms/step - loss: 2.2623\n",
            "--- Generating with seed: \"in the moral.=--a potent species of joy (and thereby the sou\"\n",
            "------ temperature: 0.2\n",
            "in the moral.=--a potent species of joy (and thereby the sou"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'function' object does not support item assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c7f9fde720e3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0msampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVJA2fWaMpBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}